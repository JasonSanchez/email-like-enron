{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def load_gutenberg_book(url, char_limit=10000, min_len_of_sections=40):\n",
    "    \"\"\"\n",
    "    Returns a list of paragraphs in the book.\n",
    "    \n",
    "    url: A url from Project Gutenberg.\n",
    "    char_limit: Amount of characters of the book to read.\n",
    "    min_len_of_sections: Each paragraph must be at least this many characters long.\n",
    "    \"\"\"\n",
    "    book = urllib.urlopen(url)\n",
    "    book_text = book.read(char_limit if char_limit else -1)\n",
    "    \n",
    "    result = []\n",
    "    for text in book_text[:char_limit].split(\"\\r\\n\\r\\n\"):\n",
    "        if len(text) >= min_len_of_sections:\n",
    "            clean_text = text.replace(\"\\r\\n\", \" \").strip()\n",
    "            result.append(clean_text)\n",
    "    \n",
    "    start_position = len(result) if len(result) < 6 else 6\n",
    "    return result[start_position:]\n",
    "\n",
    "def get_text(path):\n",
    "    encoding_options = \"ascii utf-8 utf-16 utf-32 utf-16-be utf-16-le utf-32-be utf-32-le\".split()\n",
    "    \n",
    "    for encoding in encoding_options:\n",
    "        try:\n",
    "            with open(path, encoding=encoding) as book:\n",
    "                return book.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise ValueError\n",
    "\n",
    "def extract_term(term_indicator, text, default=None, max_term_size=75):\n",
    "    term_start = text.find(term_indicator)\n",
    "    # If not found, \n",
    "    if term_start == -1:\n",
    "        term = default\n",
    "    else:\n",
    "        term_end = text.find(\"\\n\", term_start)\n",
    "        term = text[term_start+len(term_indicator):term_end].strip()\n",
    "    if term and (len(term) > max_term_size):\n",
    "        term = default\n",
    "    return term\n",
    "\n",
    "def get_author_and_title(book_text):\n",
    "    title = extract_term(\"Title:\", book_text, default=None)\n",
    "    author = extract_term(\"Author:\", book_text, default=None)\n",
    "    # Solve  for other strange author name formatting\n",
    "    for term_indicator in [\"\\n\\nby \", \"\\n\\nOF \", \"\\nOF\\n\"]:\n",
    "        if author is None:\n",
    "            author = extract_term(term_indicator, book_text[:15000], max_term_size=25)\n",
    "    return title, author\n",
    "\n",
    "def locate_beginning_of_text(title, author, text):\n",
    "    location = text.find(\"START OF THIS PROJECT GUTENBERG\") + 25\n",
    "    \n",
    "    if location < 0:\n",
    "        if title:\n",
    "            location = text.find(title)\n",
    "        if author:\n",
    "            location = text.find(author)\n",
    "            \n",
    "    return location\n",
    "\n",
    "def locate_end_of_text(text):\n",
    "    f = text.find\n",
    "    \n",
    "    search_terms = [\"End of Project Gutenberg\",\n",
    "                    \"END OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "                    \"END OF THE PROJECT GUTENBERG EBOOK\",\n",
    "                    \"End of the Project Gutenberg Etext\"]\n",
    "    \n",
    "    location = max([f(term) for term in search_terms])\n",
    "    if location < 0:\n",
    "        print(\"Fail\")\n",
    "        location = None\n",
    "    return location\n",
    "\n",
    "def parse_book(book_text, min_paragraph_characters=100):\n",
    "    \"\"\"\n",
    "    Given the text of a book, returns a list of dictionaries with the keys:\n",
    "    {title, author, contents, part, hash}\n",
    "    \"\"\"\n",
    "    parsed_book_paragraphs = []\n",
    "    title, author = get_author_and_title(book_text)\n",
    "    text_starts = locate_beginning_of_text(title, author, book_text)\n",
    "    text_ends = locate_end_of_text(book_text)\n",
    "    book_paragraphs = book_text[text_starts:text_ends].split(\"\\n\\n\")\n",
    "    for paragraph_number, raw_paragraph in enumerate(book_paragraphs):\n",
    "        paragraph = raw_paragraph.replace(\"\\n\", \" \").strip()\n",
    "        if len(paragraph) < min_paragraph_characters:\n",
    "            continue\n",
    "        if \"gutenberg\" in paragraph.lower():\n",
    "            continue\n",
    "        book_data = {\"title\": title,\n",
    "                     \"author\": author,\n",
    "                     \"contents\": paragraph,\n",
    "                     \"part\": paragraph_number}\n",
    "        parsed_book_paragraphs.append(book_data)\n",
    "    return parsed_book_paragraphs            \n",
    "\n",
    "def get_list_of_book_paths(book_directory):\n",
    "    return list(glob.iglob(book_directory + '/*.txt'))\n",
    "\n",
    "def books_to_pandas(book_directory, min_paragraph_characters=100):\n",
    "    paragraphs = []\n",
    "\n",
    "    for filename in get_list_of_book_paths(book_directory):\n",
    "        book_text = get_text(filename)\n",
    "        parsed_book = parse_book(book_text, min_paragraph_characters)\n",
    "        paragraphs.extend(parsed_book)\n",
    "    \n",
    "    return pd.DataFrame(paragraphs)\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "def cosine_similarity(new_docs, old_docs):\n",
    "    \"\"\"\n",
    "    Returns a similarity matrix where the first row is an array of\n",
    "    similarities of the first new_doc compared with each of the old\n",
    "    docs.\n",
    "    \"\"\"\n",
    "    return new_docs*old_docs.T\n",
    "\n",
    "def find_closest_matches(similarity_matrix, n_matches_to_return=1):\n",
    "    \"\"\"\n",
    "    Expects a dense array of the form [[1., .5, .2],\n",
    "                                       [.3, 1., .1],\n",
    "                                       [.2, .4, 1.]]\n",
    "    where rows correspond to similarities.\n",
    "    \"\"\"\n",
    "    top_indices = np.apply_along_axis(func1d=lambda x: x.argsort()[-n_matches_to_return:][::-1], \n",
    "                                      axis=1, \n",
    "                                      arr=similarity_matrix)\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "books = books_to_pandas(\"popular_books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>contents</th>\n",
       "      <th>part</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexandre Dumas, Pere</td>\n",
       "      <td>On the 24th of February, 1815, the look-out at...</td>\n",
       "      <td>6</td>\n",
       "      <td>THE COUNT OF MONTE CRISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexandre Dumas, Pere</td>\n",
       "      <td>As usual, a pilot put off immediately, and rou...</td>\n",
       "      <td>7</td>\n",
       "      <td>THE COUNT OF MONTE CRISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexandre Dumas, Pere</td>\n",
       "      <td>Immediately, and according to custom, the ramp...</td>\n",
       "      <td>8</td>\n",
       "      <td>THE COUNT OF MONTE CRISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexandre Dumas, Pere</td>\n",
       "      <td>The ship drew on and had safely passed the str...</td>\n",
       "      <td>9</td>\n",
       "      <td>THE COUNT OF MONTE CRISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexandre Dumas, Pere</td>\n",
       "      <td>The vague disquietude which prevailed among th...</td>\n",
       "      <td>10</td>\n",
       "      <td>THE COUNT OF MONTE CRISTO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author                                           contents  \\\n",
       "0  Alexandre Dumas, Pere  On the 24th of February, 1815, the look-out at...   \n",
       "1  Alexandre Dumas, Pere  As usual, a pilot put off immediately, and rou...   \n",
       "2  Alexandre Dumas, Pere  Immediately, and according to custom, the ramp...   \n",
       "3  Alexandre Dumas, Pere  The ship drew on and had safely passed the str...   \n",
       "4  Alexandre Dumas, Pere  The vague disquietude which prevailed among th...   \n",
       "\n",
       "   part                      title  \n",
       "0     6  THE COUNT OF MONTE CRISTO  \n",
       "1     7  THE COUNT OF MONTE CRISTO  \n",
       "2     8  THE COUNT OF MONTE CRISTO  \n",
       "3     9  THE COUNT OF MONTE CRISTO  \n",
       "4    10  THE COUNT OF MONTE CRISTO  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(max_df=.7, min_df=100, tokenizer=LemmaTokenizer()).fit_transform(books[:20000].contents)     \n",
    "similarities = cosine_similarity(vect, vect).todense()\n",
    "matches = find_closest_matches(similarities, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7872149245626546\n",
      "On the 24th of February, 1815, the look-out at Notre-Dame de la Garde signalled the three-master, the Pharaon from Smyrna, Trieste, and Naples.\n",
      "[(0.7872149245626546, '\"It is the social capital of a theatre on the boulevard, or a railroad from the Jardin des Plantes to La Rapee.\"')]\n",
      "0 [   0 2051]\n",
      "\n",
      "0.9111984962475744\n",
      "\"'The king's attorney is informed by a friend to the throne and the religions institutions of his country, that one named Edmond Dantes, mate of the ship Pharaon, this day arrived from Smyrna, after having touched at Naples and Porto-Ferrajo, has been the bearer of a letter from Murat to the usurper, and again taken charge of another letter from the usurper to the Bonapartist club in Paris. Ample corroboration of this statement may be obtained by arresting the above-mentioned Edmond Dantes, who either carries the letter for Paris about with him, or has it at his father's abode. Should it not be found in the possession of father or son, then it will assuredly be discovered in the cabin belonging to the said Dantes on board the Pharaon.'\"\n",
      "[(0.9111984962475744, '\"The king\\'s attorney is informed by a friend to the throne and religion that one Edmond Dantes, second in command on board the Pharaon, this day arrived from Smyrna, after having touched at Naples and Porto-Ferrajo, is the bearer of a letter from Murat to the usurper, and of another letter from the usurper to the Bonapartist club in Paris. Ample corroboration of this statement may be obtained by arresting the above-mentioned Edmond Dantes, who either carries the letter for Paris about with him, or has it at his father\\'s abode. Should it not be found in possession of either father or son, then it will assuredly be discovered in the cabin belonging to the said Dantes on board the Pharaon.\"')]\n",
      "399 [ 399 4489]\n",
      "\n",
      "1.0\n",
      "Se alle sei della mattina le quattro mile piastre non sono nelle mie mani, alla sette il conte Alberto avra cessato di vivere.\n",
      "[(1.0, 'Se alle sei della mattina le quattro mile piastre non sono nelle mie mani, alla sette il conte Alberto avra cessato di vivere.')]\n",
      "1934 [1940 1934]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_score = 0\n",
    "\n",
    "for new_text, old_texts in enumerate(matches[:]):\n",
    "    max_score = max([float(similarities[[new_text],[ind]]) for ind in old_texts[1:]])\n",
    "    if top_score < max_score:\n",
    "        top_score = max_score\n",
    "        print (max_score)\n",
    "        similar_texts = [(float(similarities[[new_text],[ind]]), books.contents.ix[ind]) for ind in old_texts[1:]]\n",
    "        print (books.contents.ix[new_text])\n",
    "        print (similar_texts)\n",
    "        print (new_text, old_texts)\n",
    "        print ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
